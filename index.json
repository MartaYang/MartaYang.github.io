[{"authors":null,"categories":null,"content":"I am a Master candidate at VIPL research group, Institute of Computing Technology (ICT), University of Chinese Academy of Sciences (UCAS), under the supervision of Prof. Xilin Chen and Prof. Ruiping Wang. Prior to that, I received the Bachelor of Engineering degree in Computer Science and Technology from University of Chinese Academy of Sciences (UCAS) in 2019. During that period, I also completed a student exchange program of Computer Science in École Polytechnique Fédérale de Lausanne (EPFL) in 2018.\nMy research interests lie in the field of Computer Vision. My main focus is on how to use semantic knowledge in Few-Shot Learning. I am also interested in Robotics where I have some experience in using ROS and PyRobot.\n  Download my resumé.\n","date":1641254400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1641254400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Master candidate at VIPL research group, Institute of Computing Technology (ICT), University of Chinese Academy of Sciences (UCAS), under the supervision of Prof. Xilin Chen and Prof. Ruiping Wang.","tags":null,"title":"Fengyuan Yang","type":"authors"},{"authors":["Fengyuan Yang","Ruiping Wang","Xilin Chen"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --\r","date":1641254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641254400,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://MartaYang.github.io/publication/example/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Teaching machines to recognize a new category based on few training samples especially only one remains challenging owing to the incomprehensive understanding of the novel category caused by the lack of data. However, human can learn new classes quickly even given few samples since human can tell what discriminative features should be focused on about each category based on both the visual and semantic prior knowledge. To better utilize those prior knowledge, we propose the SEmantic Guided Attention (SEGA) mechanism where the semantic knowledge is used to guide the visual perception in a top-down manner about what visual features should be paid attention to when distinguishing a category from the others. As a result, the embedding of the novel class even with few samples can be more discriminative. Concretely, a feature extractor is trained to embed few images of each novel class into a visual prototype with the help of transferring visual prior knowledge from base classes. Then we learn a network that maps semantic knowledge to category-specific attention vectors which will be used to perform feature selection to enhance the visual prototypes. Extensive experiments on miniImageNet, tieredImageNet, CIFAR-FS, and CUB indicate that our semantic guided attention realizes anticipated function and outperforms state-of-the-art results.","tags":[],"title":"SEGA: Semantic Guided Attention on Visual Prototype for Few-Shot Learning","type":"publication"},{"authors":null,"categories":null,"content":"This is an ongoing robot project named Robot Vision Exploration Project, where I am the designer and maintainer of the customized communication framework that transfers data between the robot (e.g., LoCoBot or Loomo) and functional modules (e.g., 3D Reconstruction, Scene Graph, etc.) based on ROS. In this project, I learned practical robotic tools like PyRobot to control the robots, front-end/back-end web technology to build a demonstration website, and the ability to self-study and quickly assume responsibility in teamwork.\nThis year we completed: 1) Robot entity update from Loomo to LoCoBot, the new robot has better movement stability, hardware scalability and software development; 2) Robot movement mode upgrade from passive control to active exploration. By deploying the ANS algorithm, the robot realizes autonomous positioning and real-time mapping based on image data, which truly gives the agent the motivation and ability to actively explore; 3) The cognitive model is improved from 2D scene analysis to 3D scene understanding, realizing the full mining and three-dimensional display of the semantic space relationship; 4) The lightweight improvement of the system communication architecture. We unified the data transmission in the form of ROS topics, realizing the efficient transmission of data between modules and the real-time display of the front end of the web page.\n","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"fe4a7fd64423cca87b803a61b4ce56f0","permalink":"https://MartaYang.github.io/project/locobot/","publishdate":"2021-12-20T00:00:00Z","relpermalink":"/project/locobot/","section":"project","summary":"This year we upgraded our robot to LoCoBot, a mainstream robot for robotics research, and added additional features to this Robot Vision Exploration Project. I am still the designer and maintainer of the customized communication framework and the demo website.","tags":["Robotics","ROS","PyRobot","LoCoBot"],"title":"LoCoBot Project","type":"project"},{"authors":null,"categories":null,"content":"This year, we achieved 3D real-time reconstruction in real situations, as well as multi-dimensional scene understanding using a scene graph and 3D semantic segmentation. A distributed, decentralized, and scalable module communication framework between the robot (i.e., Loomo) and functional modules (e.g., 3D Reconstruction, Scene Graph, etc.) has been built. We also implemented the function of dynamic display and analysis of each module\u0026rsquo;s results with the front end of the web page.\nThis is now an ongoing robot project named Robot Vision Exploration Project, where I am the designer and maintainer of the customized communication framework that transfers data between the robot (e.g., LoCoBot or Loomo) and functional modules (e.g., 3D Reconstruction, Scene Graph, etc.) based on ROS. In this project, I learned practical robotic tools like PyRobot to control the robots, front-end/back-end web technology to build a demonstration website, and the ability to self-study and quickly assume responsibility in teamwork.\n","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"dc49cc49712b5b503ab012125b5c7c6c","permalink":"https://MartaYang.github.io/project/loomo/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/project/loomo/","section":"project","summary":"In this project we are building intelligent system based on a real robot (i.e., Loomo). I am the designer and maintainer of the customized communication framework that transfers data between Loomo and functional modules (e.g., 3D Reconstruction, Scene Graph, etc.) based on ROS.","tags":["Robotics","ROS","Loomo"],"title":"Loomo Project","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://MartaYang.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]