[{"authors":null,"categories":null,"content":"I am currently a Ph.D. candidate at the CVML group of the National University of Singapore (NUS), supervised by Prof. Angela Yao. Before joining NUS, I earned both my Master\u0026rsquo;s and Bachelor\u0026rsquo;s degrees at the VIPL research group of the Institute of Computing Technology (ICT), University of Chinese Academy of Sciences (UCAS), under the guidance of Prof. Xilin Chen and Prof. Ruiping Wang.\nMy research interests encompass the field of Computer Vision, with a particular emphasis on Human Pose Estimation during my Ph.D. Previously, during my Master\u0026rsquo;s studies, I explored the incorporation of semantic knowledge in Few-Shot Learning.\nDownload my resum√©.\n","date":1708992000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1708992000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am currently a Ph.D. candidate at the CVML group of the National University of Singapore (NUS), supervised by Prof. Angela Yao. Before joining NUS, I earned both my Master\u0026rsquo;s and Bachelor\u0026rsquo;s degrees at the VIPL research group of the Institute of Computing Technology (ICT), University of Chinese Academy of Sciences (UCAS), under the guidance of Prof.","tags":null,"title":"Fengyuan Yang","type":"authors"},{"authors":["Fengyuan Yang","Kerui Gu","Angela Yao"],"categories":null,"content":"\r","date":1708992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708992000,"objectID":"3451ee2413f11c9f3df949eef7f36cfc","permalink":"https://MartaYang.github.io/publication/kitro/","publishdate":"2024-02-27T00:00:00Z","relpermalink":"/publication/kitro/","section":"publication","summary":"2D keypoints are commonly used as an additional cue to refine estimated 3D human meshes.  Current methods optimize the pose and shape parameters with a reprojection loss on the provided 2D keypoints. Such an approach, while simple and intuitive, has limited effectiveness because the optimal solution is hard to find in ambiguous parameter space and may sacrifice depth. Additionally, divergent gradients from distal joints complicate and deviate the refinement of proximal joints in the kinematic chain. To address these, we introduce Kinematic-Tree Rotation (KITRO), a novel mesh refinement strategy that explicitly models depth and human kinematic-tree structure. KITRO treats refinement from a bone-wise perspective.  Unlike previous methods which perform gradient-based optimizations, our method calculates bone directions in closed form.  By accounting for the 2D pose, bone length, and parent joint's depth, the calculation results in two possible directions for each child joint. We then use a decision tree to trace binary choices for all bones along the human skeleton's kinematic-tree to select the most probable hypothesis. Our experiments across various datasets and baseline models demonstrate that KITRO significantly improves 3D joint estimation accuracy and achieves an ideal 2D fit simultaneously.","tags":[],"title":"KITRO: Refining Human Mesh by 2D Clues and Kinematic-tree Rotation","type":"publication"},{"authors":["Fengyuan Yang","Ruiping Wang","Xilin Chen"],"categories":null,"content":"\r","date":1672790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672790400,"objectID":"d53d88d6f8382abdb99ae0d80949029a","permalink":"https://MartaYang.github.io/publication/lpe/","publishdate":"2023-01-04T00:00:00Z","relpermalink":"/publication/lpe/","section":"publication","summary":"The ability of few-shot learning (FSL) is a basic requirement of intelligent agent learning in the open visual world. However, existing deep learning systems rely too heavily on large numbers of training samples, making it hard to learn new categories efficiently from limited size of training data. Two key challenges of FSL are insufficient comprehension and imperfect modeling of the few-shot novel class. For insufficient visual comprehension, semantic knowledge which is information from other modalities can help replenish the understanding of novel classes. But even so, most works still suffer from the second challenge because the single global class prototype they adopted is extremely unstable and imperfect given the larger intra-class variation and harder inter-class discrimination in FSL scenario. Thus, we propose to represent each class by its several different parts with the help of class semantic knowledge. Since we can never pre-define parts for unknown novel classes, we embed them in a latent manner. Concretely, we train a generator that takes the class semantic knowledge as input and outputs several filters of class-specific semantic latent parts. By applying each part filter, our model can pay attention to corresponding local regions containing each part. At the inference stage, the classification is conducted by comparing the similarities between those parts. Experiments on several FSL benchmarks demonstrate the effectiveness of our proposed method and show its potential to go beyond class recognition to class understanding. Furthermore, we also find when semantic knowledge is more visualized and customized, it will be more helpful in the FSL task.","tags":[],"title":"Semantic Guided Latent Parts Embedding for Few-Shot Learning","type":"publication"},{"authors":["Fengyuan Yang","Ruiping Wang","Xilin Chen"],"categories":null,"content":"\r","date":1641254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641254400,"objectID":"6f92499e0f3ad65aa8c8556b9041d843","permalink":"https://MartaYang.github.io/publication/sega/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/publication/sega/","section":"publication","summary":"Teaching machines to recognize a new category based on few training samples especially only one remains challenging owing to the incomprehensive understanding of the novel category caused by the lack of data. However, human can learn new classes quickly even given few samples since human can tell what discriminative features should be focused on about each category based on both the visual and semantic prior knowledge. To better utilize those prior knowledge, we propose the SEmantic Guided Attention (SEGA) mechanism where the semantic knowledge is used to guide the visual perception in a top-down manner about what visual features should be paid attention to when distinguishing a category from the others. As a result, the embedding of the novel class even with few samples can be more discriminative. Concretely, a feature extractor is trained to embed few images of each novel class into a visual prototype with the help of transferring visual prior knowledge from base classes. Then we learn a network that maps semantic knowledge to category-specific attention vectors which will be used to perform feature selection to enhance the visual prototypes. Extensive experiments on miniImageNet, tieredImageNet, CIFAR-FS, and CUB indicate that our semantic guided attention realizes anticipated function and outperforms state-of-the-art results.","tags":[],"title":"SEGA: Semantic Guided Attention on Visual Prototype for Few-Shot Learning","type":"publication"},{"authors":null,"categories":null,"content":"This is an ongoing robot project named Robot Vision Exploration Project, where I am the designer and maintainer of the customized communication framework that transfers data between the robot (e.g., LoCoBot or Loomo) and functional modules (e.g., 3D Reconstruction, Scene Graph, etc.) based on ROS. In this project, I learned practical robotic tools like PyRobot to control the robots, front-end/back-end web technology to build a demonstration website, and the ability to self-study and quickly assume responsibility in teamwork.\nThis year we completed: 1) Robot entity update from Loomo to LoCoBot, the new robot has better movement stability, hardware scalability and software development; 2) Robot movement mode upgrade from passive control to active exploration. By deploying the ANS algorithm, the robot realizes autonomous positioning and real-time mapping based on image data, which truly gives the agent the motivation and ability to actively explore; 3) The cognitive model is improved from 2D scene analysis to 3D scene understanding, realizing the full mining and three-dimensional display of the semantic space relationship; 4) The lightweight improvement of the system communication architecture. We unified the data transmission in the form of ROS topics, realizing the efficient transmission of data between modules and the real-time display of the front end of the web page.\n","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"fe4a7fd64423cca87b803a61b4ce56f0","permalink":"https://MartaYang.github.io/project/locobot/","publishdate":"2021-12-20T00:00:00Z","relpermalink":"/project/locobot/","section":"project","summary":"This year we upgraded our robot to LoCoBot, a mainstream robot for robotics research, and added additional features to this Robot Vision Exploration Project. I am still the designer and maintainer of the customized communication framework and the demo website.","tags":["Robotics","ROS","PyRobot","LoCoBot"],"title":"LoCoBot Project","type":"project"},{"authors":null,"categories":null,"content":"This year, we achieved 3D real-time reconstruction in real situations, as well as multi-dimensional scene understanding using a scene graph and 3D semantic segmentation. A distributed, decentralized, and scalable module communication framework between the robot (i.e., Loomo) and functional modules (e.g., 3D Reconstruction, Scene Graph, etc.) has been built. We also implemented the function of dynamic display and analysis of each module\u0026rsquo;s results with the front end of the web page.\nThis is now an ongoing robot project named Robot Vision Exploration Project, where I am the designer and maintainer of the customized communication framework that transfers data between the robot (e.g., LoCoBot or Loomo) and functional modules (e.g., 3D Reconstruction, Scene Graph, etc.) based on ROS. In this project, I learned practical robotic tools like PyRobot to control the robots, front-end/back-end web technology to build a demonstration website, and the ability to self-study and quickly assume responsibility in teamwork.\n","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"dc49cc49712b5b503ab012125b5c7c6c","permalink":"https://MartaYang.github.io/project/loomo/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/project/loomo/","section":"project","summary":"In this project we are building intelligent system based on a real robot (i.e., Loomo). I am the designer and maintainer of the customized communication framework that transfers data between Loomo and functional modules (e.g., 3D Reconstruction, Scene Graph, etc.) based on ROS.","tags":["Robotics","ROS","Loomo"],"title":"Loomo Project","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://MartaYang.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]